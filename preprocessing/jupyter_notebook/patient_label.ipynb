{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c24195",
   "metadata": {},
   "source": [
    "# step0: preprocessing task label (readminssion and expired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd42102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import sklearn.model_selection as ms\n",
    "encounter_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "575e4e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncounterInfo(object):\n",
    "\n",
    "    def __init__(self, patient_id, encounter_id, encounter_timestamp, expired,\n",
    "                 readmission, los_3day, los_7day, Death30, Death180, Death365, admission_type=99, marital_status=99):\n",
    "        self.patient_id = patient_id\n",
    "        self.encounter_id = encounter_id\n",
    "        self.encounter_timestamp = encounter_timestamp\n",
    "        self.expired = expired\n",
    "        self.readmission = readmission\n",
    "        self.admission_type = admission_type\n",
    "        self.marital_status = marital_status\n",
    "        self.los_3day = los_3day\n",
    "        self.los_7day = los_7day\n",
    "        self.gender = ''\n",
    "        self.dx_ids = []\n",
    "        self.dx_ids_lvl1 = []\n",
    "        self.dx_names = []\n",
    "        self.dx_labels = []\n",
    "        self.rx_ids = []\n",
    "        self.rx_names = []\n",
    "        self.lab_ids = []\n",
    "        self.lab_names = []\n",
    "        self.microbiology_ids = []\n",
    "        self.microbiology_names = []\n",
    "        self.physicals = []\n",
    "        self.procedures_ids = []\n",
    "        self.procedures_names = []\n",
    "        self.Death30 = Death30\n",
    "        self.Death180 = Death180\n",
    "        self.Death365 = Death365\n",
    "\n",
    "\n",
    "def minNums(startTime, endTime):\n",
    "    '''计算两个时间点之间的分钟数'''\n",
    "    # 处理格式,加上秒位\n",
    "    startTime1 = startTime  # + ':00'\n",
    "    endTime1 = endTime  # + ':00'\n",
    "    # 计算分钟数\n",
    "    startTime2 = datetime.datetime.strptime(startTime1, \"%Y-%m-%d %H:%M:%S\")\n",
    "    endTime2 = datetime.datetime.strptime(endTime1, \"%Y-%m-%d %H:%M:%S\")\n",
    "    seconds = (endTime2 - startTime2).seconds\n",
    "    # 来获取时间差中的秒数。注意，seconds获得的秒只是时间差中的小时、分钟和秒部分的和，并没有包含时间差的天数（既是两个时间点不是同一天，失效）\n",
    "    total_seconds = (endTime2 - startTime2).total_seconds()\n",
    "    # 来获取准确的时间差，并将时间差转换为秒\n",
    "    # print(total_seconds)\n",
    "    mins = total_seconds / 60\n",
    "    return int(mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8a465f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_seed 用于随机生成\n",
    "def process_admission(admission_file, icustays_file, patients_file, encounter_dict, max_patient_num=500, hour_threshold=24):\n",
    "\n",
    "    count = 0\n",
    "    patient_dict = {}\n",
    "    admission_list = []\n",
    "\n",
    "    patient_base_dict = {}\n",
    "    inff = open(patients_file, 'r')\n",
    "    for line in csv.DictReader(inff):\n",
    "        patient_id = line['SUBJECT_ID']\n",
    "        gender = line['GENDER']\n",
    "        birthday = line['DOB']\n",
    "        if patient_id not in patient_base_dict:\n",
    "            patient_base_dict[patient_id] = []\n",
    "        patient_base_dict[patient_id].append((gender,birthday))\n",
    "    inff.close()\n",
    "\n",
    "\n",
    "    # --------------- icustays_file --------------------\n",
    "#     patient_icu_dict = {}\n",
    "#     inff = open(icustays_file, 'r')\n",
    "#     for line in csv.DictReader(inff):\n",
    "#         FIRST_CAREUNIT = line['FIRST_CAREUNIT']\n",
    "#         LAST_CAREUNIT = line['LAST_CAREUNIT']\n",
    "#         icu_intime = line['INTIME']\n",
    "#         icu_outtime = line['OUTTIME']\n",
    "#         patient_id = line['SUBJECT_ID']\n",
    "#         encounter_id = line['HADM_ID']\n",
    "#         birthday = patient_base_dict[patient_id][0][1]\n",
    "#         age = minNums(birthday, icu_intime)/(24. * 60 * 365)\n",
    "\n",
    "#         ## ICU stay以及年龄大于18\n",
    "#         if FIRST_CAREUNIT == LAST_CAREUNIT and LAST_CAREUNIT=='MICU' and age > 18:\n",
    "#             if patient_id not in patient_icu_dict:\n",
    "#                 patient_icu_dict[patient_id] = []\n",
    "#             patient_icu_dict[patient_id].append(encounter_id)\n",
    "#     inff.close()\n",
    "\n",
    "\n",
    "    # --------------- admission_file --------------------\n",
    "    inff = open(admission_file, 'r')\n",
    "    for line in csv.DictReader(inff):\n",
    "        if count % 1000 == 0:\n",
    "            sys.stdout.write('%d\\r' % count)\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # if count == max_admission_num:\n",
    "        #     break\n",
    "\n",
    "        patient_id = line['SUBJECT_ID']\n",
    "        encounter_id = line['HADM_ID']\n",
    "        admittime = line['ADMITTIME']\n",
    "        dischtime = line['DISCHTIME']\n",
    "\n",
    "        # encounter_timestamp = -int(line['hospitaladmitoffset'])\n",
    "        encounter_timestamp = minNums(admittime, dischtime)\n",
    "        # encounter_timestamp：number of minutes from unit admit time that the patient was admitted to the hospital\n",
    "\n",
    "#         # 只统计ICU的记录\n",
    "#         if patient_id in patient_icu_dict.keys():\n",
    "#             if encounter_id in patient_icu_dict[patient_id]:\n",
    "        if patient_id not in patient_dict:\n",
    "            patient_dict[patient_id] = []\n",
    "        patient_dict[patient_id].append((admittime, encounter_timestamp, encounter_id,dischtime))\n",
    "\n",
    "        count += 1\n",
    "    inff.close()\n",
    "\n",
    "    # 随机的数量不能大于总的患者数量\n",
    "    if max_patient_num > len(patient_dict):\n",
    "        max_patient_num = len(patient_dict)\n",
    "    # 随机选取 患者\n",
    "    patient_random_keys = random.sample(patient_dict.keys(), max_patient_num)\n",
    "    patient_random_del_keys = []\n",
    "\n",
    "    for patient_id in patient_dict.keys():\n",
    "        if patient_id not in patient_random_keys:\n",
    "            patient_random_del_keys.append(patient_id)\n",
    "    # 删除不在随机范围内的患者记录\n",
    "    for patient_random_del_key in patient_random_del_keys:\n",
    "        del patient_dict[patient_random_del_key]\n",
    "    # admission_list，只存储随机到的患者的就诊记录\n",
    "    for patient_id, encounter_ids in patient_dict.items():\n",
    "        for encounter_id in encounter_ids:\n",
    "            if encounter_id[2] not in admission_list:\n",
    "                admission_list.append(encounter_id[2])\n",
    "\n",
    "    # sort\n",
    "    patient_dict_sorted = {}\n",
    "    for patient_id, time_enc_tuples in patient_dict.items():\n",
    "        # print(time_enc_tuples)\n",
    "        patient_dict_sorted[patient_id] = sorted(time_enc_tuples, reverse=False)\n",
    "\n",
    "    # enc_readmission_dict 判断该患者是否是重新入院的\n",
    "    enc_readmission_dict = {}\n",
    "    for patient_id, time_enc_tuples in patient_dict_sorted.items():\n",
    "        for time_enc_tuple in time_enc_tuples[:-1]:\n",
    "            enc_id = time_enc_tuple[2]\n",
    "            enc_readmission_dict[enc_id] = 1\n",
    "        last_enc_id = time_enc_tuples[-1][2]\n",
    "        enc_readmission_dict[last_enc_id] = 0\n",
    "\n",
    "\n",
    "    # enc_readmission_dict 判断该患者是否是重新入院,在出院30天内\n",
    "#     enc_readmission_dict = {}\n",
    "#     for patient_id, time_enc_tuples in patient_dict_sorted.items(): \n",
    "#         flag=0\n",
    "#         handle_flag = 0\n",
    "#         for time_enc_tuple in time_enc_tuples:\n",
    "#             enc_readmission_dict[time_enc_tuple[2]] = 0 # 先默认本次入院记录的next_readmission_flag 为0，后面再更新\n",
    "#             if handle_flag == 1:  #第一次循环不处理下次入院时间，从第二次入院记录开始，更新上次入院的next_readmission day\n",
    "#                 readminssionday = 999\n",
    "#                 encounter_timestamp = minNums(last_time_enc_tuple[3], time_enc_tuple[0]) # 上次出院到本次入院的时间\n",
    "#                 readminssionday =  encounter_timestamp/(24. * 60)\n",
    "#                 if readminssionday<=30:\n",
    "#                     enc_readmission_dict[last_time_enc_tuple[2]] = 1\n",
    "#                 else:\n",
    "#                     enc_readmission_dict[last_time_enc_tuple[2]] = 0\n",
    "# #                 print(patient_id,last_time_enc_tuple[2],'>>>',last_time_enc_tuple[0],last_time_enc_tuple[3],'>>>',readminssionday)\n",
    "#             handle_flag = 1\n",
    "#             last_time_enc_tuple = time_enc_tuple\n",
    "            \n",
    "\n",
    "    inff = open(admission_file, 'r')\n",
    "    count = 0\n",
    "    for line in tqdm(csv.DictReader(inff)):\n",
    "        if line['HADM_ID'] in admission_list:\n",
    "            patient_id = line['SUBJECT_ID']\n",
    "            encounter_id = line['HADM_ID']\n",
    "\n",
    "            admittime = line['ADMITTIME']\n",
    "            dischtime = line['DISCHTIME']\n",
    "            deathtime = line['DEATHTIME']\n",
    "\n",
    "            encounter_timestamp = minNums(admittime, dischtime)\n",
    "            if deathtime is not None and deathtime!='':  \n",
    "                death_timestamp = minNums(admittime, deathtime)\n",
    "            else:\n",
    "                death_timestamp=0\n",
    "\n",
    "            # hospital_expire_flag：This is a binary flag which indicates whether the patient died within the given hospitalization.\n",
    "            # ---------------------： 1 indicates death in the hospital, and 0 indicates survival to hospital discharge.\n",
    "            hospital_expire_flag = line['HOSPITAL_EXPIRE_FLAG']\n",
    "            duration_minute = encounter_timestamp\n",
    "            losday =  duration_minute/(24. * 60)\n",
    "            deathday = death_timestamp/(24. * 60)\n",
    "            \n",
    "            expired = 1 if hospital_expire_flag == '1' else 0\n",
    "            readmission = 1 if enc_readmission_dict[encounter_id] ==1 else 0\n",
    "            los_3day = 1 if losday > 3 else 0\n",
    "            los_7day = 1 if losday > 7 else 0\n",
    "            \n",
    "            Death30 = 1 if 30>= deathday >0 else 0\n",
    "            Death180 = 1 if 180>= deathday >0 else 0\n",
    "            Death365 = 1 if 365>= deathday >0 else 0\n",
    "            \n",
    "        \n",
    "            if duration_minute < 60. * hour_threshold:\n",
    "                continue\n",
    "\n",
    "            ei = EncounterInfo(patient_id, encounter_id, encounter_timestamp, expired,\n",
    "                               readmission, los_3day, los_7day, Death30, Death180, Death365)\n",
    "            if encounter_id in encounter_dict:\n",
    "                print('Duplicate encounter ID!!')\n",
    "                sys.exit(0)\n",
    "            encounter_dict[encounter_id] = ei\n",
    "            count = count + 1\n",
    "    inff.close()\n",
    "\n",
    "    print('Accepted Patients: {}'.format(max_patient_num))\n",
    "    print('Accepted admissions: {}'.format(count))\n",
    "    print('')\n",
    "    return encounter_dict, admission_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e24536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patients(patients_file, encounter_dict):\n",
    "    count = 0\n",
    "    enc_dict = encounter_dict\n",
    "    inff = open(patients_file, 'r')\n",
    "    for line in csv.DictReader(inff):\n",
    "        patient_id = line['SUBJECT_ID']\n",
    "        gender = line['GENDER']\n",
    "        for _, enc in enc_dict.items():\n",
    "            if enc.patient_id == patient_id:\n",
    "                enc.gender = gender\n",
    "            count += 1\n",
    "    inff.close()\n",
    "\n",
    "    print('Accepted admissions: %d' % count)\n",
    "    print('')\n",
    "    return encounter_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9fe5cd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_patient_num:100000\n",
      "Processing ADMISSIONS.csv\n",
      "58000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58976it [00:45, 1290.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accepted Patients: 46520\n",
      "Accepted admissions: 56684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input_path = argv[1]\n",
    "# output_path = argv[2]\n",
    "# input_path = '/home/caoyu/jupyterNotebook/data_test'\n",
    "# output_path = '/home/caoyu/project/healthRecords/data/mimiciv'\n",
    "\n",
    "input_path = '/home/caoyu/project/GraphCLHealth/data/mimiciii'\n",
    "output_path = '/home/caoyu/project/GraphCLHealth/processed_data/mimiciii'\n",
    "minimum_cnt = 5\n",
    "max_patient_num = 100000\n",
    "print('max_patient_num:' + str(max_patient_num))\n",
    "\n",
    "\n",
    "flag_test_flag = 1 # whether to use the test files for debugging\n",
    "icd_diagnoses_file = input_path + '/D_ICD_DIAGNOSES.csv'\n",
    "icd_procedures_file = input_path + '/D_ICD_PROCEDURES.csv'\n",
    "icd_labItems_file = input_path + '/D_LABITEMS.csv'\n",
    "\n",
    "\n",
    "if flag_test_flag == 0:\n",
    "    admission_file = input_path + '/ADMISSIONS10.csv'\n",
    "    diagnosis_file = input_path + '/DIAGNOSES_ICD10.csv'\n",
    "    procedures_file = input_path + '/PROCEDURES_ICD10.csv'\n",
    "    labevents_file = input_path + '/LABEVENTS10.csv'\n",
    "    microbiology_file = input_path + '/microbiologyevents10.csv'\n",
    "    patients_file = input_path + '/PATIENTS.csv'\n",
    "    icustays_file = input_path + '/ICUSTAYS10.csv'\n",
    "else:\n",
    "    admission_file = input_path + '/ADMISSIONS.csv'\n",
    "    diagnosis_file = input_path + '/DIAGNOSES_ICD.csv'\n",
    "    procedures_file = input_path + '/PROCEDURES_ICD.csv'\n",
    "    labevents_file = input_path + '/LABEVENTS.csv'\n",
    "    microbiology_file = input_path + '/microbiologyevents.csv'\n",
    "    patients_file = input_path + '/PATIENTS.csv'\n",
    "    icustays_file = input_path + '/ICUSTAYS.csv'\n",
    "# 调试用的文件\n",
    "\n",
    "encounter_dict = {}\n",
    "\n",
    "# max_admission_nun 处理最大的住院患者流水数量\n",
    "print('Processing ADMISSIONS.csv')\n",
    "encounter_dict, admission_list = process_admission(admission_file, icustays_file, patients_file, encounter_dict, max_patient_num, hour_threshold=24)\n",
    "# print('Processing PATIENTS.csv')\n",
    "# encounter_dict = process_patients(patients_file, encounter_dict)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2702bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "expired = {}\n",
    "readmission = {}\n",
    "death30 = {}\n",
    "death180 = {}\n",
    "death365 = {}\n",
    "for key in encounter_dict.keys():\n",
    "    if encounter_dict[key].encounter_id not in expired:\n",
    "        expired[encounter_dict[key].encounter_id] = encounter_dict[key].expired\n",
    "    if encounter_dict[key].encounter_id not in readmission:\n",
    "        readmission[encounter_dict[key].encounter_id] = encounter_dict[key].readmission\n",
    "    if encounter_dict[key].encounter_id not in death30:\n",
    "        death30[encounter_dict[key].encounter_id] = encounter_dict[key].Death30\n",
    "    if encounter_dict[key].encounter_id not in death180:\n",
    "        death180[encounter_dict[key].encounter_id] = encounter_dict[key].Death180      \n",
    "    if encounter_dict[key].encounter_id not in death30:\n",
    "        death365[encounter_dict[key].encounter_id] = encounter_dict[key].Death365 \n",
    "        \n",
    "#     print(key,encounter_dict[key].encounter_id,encounter_dict[key].expired,encounter_dict[key].readmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f9fa10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readmission 12134\n",
      "death30 4492\n",
      "death180 4875\n",
      "expired 4883\n",
      "56684\n",
      "56684\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for pa in readmission:\n",
    "    if readmission[pa]==1:\n",
    "        cnt+=1\n",
    "print('readmission',cnt)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for pa in death30:\n",
    "    if death30[pa]==1:\n",
    "        cnt+=1\n",
    "print('death30',cnt)\n",
    "\n",
    "cnt = 0\n",
    "for pa in death180:\n",
    "    if death180[pa]==1:\n",
    "        cnt+=1\n",
    "print('death180',cnt)\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for pa in expired:\n",
    "    if expired[pa]==1:\n",
    "        cnt+=1\n",
    "print('expired', cnt)\n",
    "\n",
    "#     print(death180[pa])\n",
    "print (len(expired))\n",
    "print (len(readmission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b1fa37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07924634817585209\n"
     ]
    }
   ],
   "source": [
    "print(4492/56684)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bbb0224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05777644485216287\n"
     ]
    }
   ],
   "source": [
    "print(3275/56684)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d821215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56684\n"
     ]
    }
   ],
   "source": [
    "print(len(expired))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fcdb953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56684\n"
     ]
    }
   ],
   "source": [
    "print(len(readmission))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22281ec5",
   "metadata": {},
   "source": [
    "# 5. Convert DB for ReAdmPred task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "272bc177",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "out_db_name = 'dx,prx'\n",
    "db_name = 'dxprx'\n",
    "size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']\n",
    "SOURCE_PATH = '/home/caoyu/project/MultiModalMed/preprocessing/legacy/13-01-2021'\n",
    "knowmix_PATH='/home/caoyu/project/MultiModalMed/gtx/data/knowmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08ee7298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### NoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 338373.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 1\n",
      "##### NoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 319176.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 1\n",
      "##### NoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 328836.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 620616.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 1\n",
      "##### UniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 578006.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 1\n",
      "##### UniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 615496.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 349362.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 1\n",
      "##### UnifiedNoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 338591.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 1\n",
      "##### UnifiedNoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 345295.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 301590.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 1\n",
      "##### UnifiedUniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 269600.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 1\n",
      "##### UnifiedUniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 301185.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n"
     ]
    }
   ],
   "source": [
    "for db_type in DB_type:\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1]) + NUM_SPECIAL_TOKENS: line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'{SOURCE_PATH}/{db_name}', 'entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v: k.split('\\t')[0].split('^^')[0] for k, v in\n",
    "             torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        print('#####',db_type,' -- ',split)\n",
    "        db = torch.load(f'{knowmix_PATH}/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in readmission:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(readmission[hadm_id])\n",
    "                    global_label.append(readmission[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "#             if idx >100:\n",
    "#                 break\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/readm/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/readm/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"20th sample\",[id2entity[x] for x in db_new['input'][20][1:10]])\n",
    "#         print(f\"100th sample\", [id2entity[x] for x in db_new['input'][0:20]])\n",
    "        print(f\"20th label\", db_new['label'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f2d836",
   "metadata": {},
   "source": [
    "# 5. Convert DB for expired task （180）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f8b5440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "out_db_name = 'dx,prx'\n",
    "db_name = 'dxprx'\n",
    "size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']\n",
    "SOURCE_PATH = '/home/caoyu/project/MultiModalMed/preprocessing/legacy/13-01-2021'\n",
    "knowmix_PATH='/home/caoyu/project/MultiModalMed/gtx/data/knowmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3b72408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### NoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 196609.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 0\n",
      "##### NoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 260160.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 0\n",
      "##### NoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 271475.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 620032.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 584571.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 617172.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 334000.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 320775.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 132979.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 304359.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 249779.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 267093.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n"
     ]
    }
   ],
   "source": [
    "for db_type in DB_type:\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1]) + NUM_SPECIAL_TOKENS: line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'{SOURCE_PATH}/{db_name}', 'entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v: k.split('\\t')[0].split('^^')[0] for k, v in\n",
    "             torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        print('#####',db_type,' -- ',split)\n",
    "        db = torch.load(f'{knowmix_PATH}/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in death180:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(death180[hadm_id])\n",
    "                    global_label.append(death180[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "#             if idx >100:\n",
    "#                 break\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/Death180/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/Death180/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"20th sample\",[id2entity[x] for x in db_new['input'][20][1:10]])\n",
    "#         print(f\"100th sample\", [id2entity[x] for x in db_new['input'][0:20]])\n",
    "        print(f\"20th label\", db_new['label'][20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e97f63",
   "metadata": {},
   "source": [
    "# 5. Convert DB for expired task （30）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa20cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from numpy.random import choice\n",
    "from tqdm import tqdm\n",
    "out_db_name = 'dx,prx'\n",
    "db_name = 'dxprx'\n",
    "size = 2000\n",
    "DB_type = ['NoKGenc','UniKGenc','UnifiedNoKGenc','UnifiedUniKGenc']\n",
    "SPLIT = ['train','valid','test']\n",
    "SOURCE_PATH = '/home/caoyu/project/MultiModalMed/preprocessing/legacy/13-01-2021'\n",
    "knowmix_PATH='/home/caoyu/project/MultiModalMed/gtx/data/knowmix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0fcba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### NoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 222512.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 0\n",
      "##### NoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 310482.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 0\n",
      "##### NoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 321920.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 574615.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "20th sample ['</hadm_id/129414>', '</diagnoses/406607>', '</diagnoses/406605>', '</diagnoses/406616>', '</diagnoses/406614>', '</diagnoses/406615>', '</diagnoses/406613>', '</diagnoses/406606>', '</diagnoses/406611>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 566721.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "20th sample ['</hadm_id/167090>', '</diagnoses/67993>', '</diagnoses/67990>', '</procedures/28197>', '</procedures/28192>', '</diagnoses/67995>', '</diagnoses/67996>', '</procedures/28190>', '</diagnoses/67992>']\n",
      "20th label 0\n",
      "##### UniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 592164.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "20th sample ['</hadm_id/185204>', '</diagnoses/299964>', '</diagnoses/299958>', '</diagnoses/299967>', '</diagnoses/299965>', '</diagnoses/299961>', '</diagnoses/299963>', '</diagnoses/299962>', '</procedures/109432>']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 338582.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 323148.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedNoKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 230241.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28915it [00:00, 302928.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,train 28358\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 261743.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,valid 1957\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses']\n",
      "20th label 0\n",
      "##### UnifiedUniKGenc  --  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:00, 291980.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dxprx,2000,test 1961\n",
      "*** Saved! ***\n",
      "20th sample ['hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures']\n",
      "20th label 0\n"
     ]
    }
   ],
   "source": [
    "for db_type in DB_type:\n",
    "    if \"Unified\" not in db_type:\n",
    "        NUM_SPECIAL_TOKENS = 3\n",
    "        id2label = torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/id2label')\n",
    "        label2id = {v:k for k,v in id2label.items()}\n",
    "        id2entity = {int(line.split('\\t')[1]) + NUM_SPECIAL_TOKENS: line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(f'{SOURCE_PATH}/{db_name}', 'entity2id.txt')).read().splitlines()[1:]}\n",
    "        label2entity = {k:id2entity[v] for k,v in label2id.items()}\n",
    "    else:\n",
    "        id2entity = {v: k.split('\\t')[0].split('^^')[0] for k, v in\n",
    "             torch.load(f'{SOURCE_PATH}/{db_name}_{db_type}/unified_node').items()}\n",
    "\n",
    "    if db_type == 'NoKGenc':\n",
    "        global_label = list()\n",
    "    idx = 0\n",
    "    for split in SPLIT:\n",
    "        print('#####',db_type,' -- ',split)\n",
    "        db = torch.load(f'{knowmix_PATH}/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "        db_new = dict()\n",
    "        for k in db:\n",
    "            if k in ['label_mask','rc_index']:\n",
    "                continue\n",
    "            db_new[k] = list()\n",
    "        for in_db_idx, _input in tqdm(enumerate(db['input'])):\n",
    "            if db_type == 'NoKGenc':\n",
    "                hadm_id = id2entity[_input[1]].split('/')[-1].replace('>','')\n",
    "                if hadm_id in death30:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(death30[hadm_id])\n",
    "                    global_label.append(death30[hadm_id])\n",
    "                else:\n",
    "                    global_label.append(None)\n",
    "            else:\n",
    "                if global_label[idx] is not None:\n",
    "                    for k in db_new:\n",
    "                        if k not in ['label','label_mask','rc_index']:\n",
    "                            db_new[k].append(db[k][in_db_idx])\n",
    "                    db_new['label'].append(global_label[idx])\n",
    "            idx += 1\n",
    "#             if idx >100:\n",
    "#                 break\n",
    "        print(f'{db_name},{size},{split}',len(db_new['label']))\n",
    "        if \"Unified\" in db_type:\n",
    "            os.makedirs(f'../gtx/data/Death30/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}', exist_ok=True)\n",
    "            torch.save(db_new,f'../gtx/data/Death30/knowmix/{out_db_name}_{size}/{out_db_name}_{db_type}/{split}/db')\n",
    "            print(\"*** Saved! ***\")\n",
    "        print(f\"20th sample\",[id2entity[x] for x in db_new['input'][20][1:10]])\n",
    "#         print(f\"100th sample\", [id2entity[x] for x in db_new['input'][0:20]])\n",
    "        print(f\"20th label\", db_new['label'][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e855cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kg_txt",
   "language": "python",
   "name": "torch_kg_txt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
