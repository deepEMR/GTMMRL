{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin sampling ...\n",
      "Processing DEMOGRAPHIC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caoyu/anaconda3/envs/torch_kg_txt/lib/python3.7/site-packages/ipykernel_launcher.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# admissions : 32699\n",
      "32699\n",
      "Processing DIAGNOSES\n",
      "Processing PROCEDURES>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100%\n",
      "Done!>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>]100%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import torch\n",
    "import shutil\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# from utils.evaluation.process_mimic_db.utils import *\n",
    "from utils.evaluation.process_mimic_db.utils import *\n",
    "from utils.evaluation.process_mimic_db.process_tables import *\n",
    "\n",
    "# Specify the path to the downloaded MIMIC III data\n",
    "data_dir = 'mimic_db'\n",
    "# Path to the generated mimic.db. No need to update.\n",
    "out_dir = 'result-dxprx'\n",
    "PJT_ROOT_PATH = out_dir\n",
    "db = 'dxprx'\n",
    "\n",
    "\n",
    "# Generate five tables and the database with all admissions\n",
    "# if os.path.exists(out_dir):\n",
    "#     shutil.rmtree(out_dir)\n",
    "# os.mkdir(out_dir)\n",
    "'''\n",
    "conn = sqlite3.connect(os.path.join(out_dir, 'mimic_all.db'))\n",
    "build_demographic_table(data_dir, out_dir, conn)\n",
    "build_diagnoses_table(data_dir, out_dir, conn)\n",
    "build_procedures_table(data_dir, out_dir, conn)\n",
    "build_prescriptions_table(data_dir, out_dir, conn)\n",
    "build_lab_table(data_dir, out_dir, conn)\n",
    "'''\n",
    "\n",
    "'''\n",
    "1. We did not emumerate all possible questions about MIMIC III.\n",
    "MIMICSQL data is generated based on the patient information \n",
    "related to 100 randomly selected admissions.\n",
    "2. The following codes are used for sampling the admissions \n",
    "from the large database. \n",
    "3. The parameter 'random_state=0' in line 41 will provide you \n",
    "the same set of sampled admissions and the same database as we used.\n",
    "'''\n",
    "\n",
    "print('Begin sampling ...')\n",
    "# DEMOGRAPHIC\n",
    "print('Processing DEMOGRAPHIC')\n",
    "conn = sqlite3.connect(os.path.join(out_dir, 'mimic.db'))\n",
    "#data_demo = pandas.read_csv(os.path.join(out_dir, \"DEMOGRAPHIC.csv\"))\n",
    "data_demo = pandas.read_csv(os.path.join(data_dir, \"DEMOGRAPHIC.csv\"))\n",
    "\n",
    "# p_sections 获取的h_adm_list 是从NOTEEVENTS.csv 过滤出的\n",
    "h_adm_list = [elem[0] for elem in torch.load(f'result-{db}/p_sections')]\n",
    "# h_adm_list_int  = [int(i) for i in h_adm_list]\n",
    "h_adm_list_int  = [int(i) for i in h_adm_list]\n",
    "data_demo_sample = data_demo[data_demo['HADM_ID'].isin(h_adm_list_int)]\n",
    "for k, v in data_demo_sample.dtypes.items():\n",
    "    data_demo_sample[k] = data_demo_sample[k].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "print(f'# admissions : {len(data_demo_sample)}')\n",
    "#data_demo_sample.to_sql('DEMOGRAPHIC', conn, if_exists='replace', index=False)\n",
    "sampled_id = data_demo_sample['HADM_ID'].values\n",
    "print(len(sampled_id))\n",
    "\n",
    "\n",
    "## 此处定义db的类型，\n",
    "if 'dx' in db:\n",
    "    # DIAGNOSES\n",
    "    print('Processing DIAGNOSES')\n",
    "    data_input = pandas.read_csv(os.path.join(data_dir, \"DIAGNOSES.csv\"))\n",
    "    data_filter = []\n",
    "    cnt = 0\n",
    "    for itm in sampled_id:\n",
    "        msg = 'HADM_ID=='+str(itm)\n",
    "        data_filter.append(data_input.query(msg))\n",
    "        cnt += 1\n",
    "        show_progress(cnt, len(sampled_id))\n",
    "    data_out = pandas.concat(data_filter, ignore_index=True)\n",
    "    for k, v in data_out.dtypes.items():\n",
    "        data_out[k] = data_out[k].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    data_out.to_sql('DIAGNOSES', conn, if_exists='replace', index=False)\n",
    "\n",
    "if 'prx' in db:\n",
    "    # PROCEDURES\n",
    "    print('Processing PROCEDURES')\n",
    "    data_input = pandas.read_csv(os.path.join(data_dir, \"PROCEDURES.csv\"))\n",
    "    data_filter = []\n",
    "    cnt = 0\n",
    "    for itm in sampled_id:\n",
    "        msg = 'HADM_ID=='+str(itm)\n",
    "        data_filter.append(data_input.query(msg))\n",
    "        cnt += 1\n",
    "        show_progress(cnt, len(sampled_id))\n",
    "    data_out = pandas.concat(data_filter, ignore_index=True)\n",
    "    for k, v in data_out.dtypes.items():\n",
    "        data_out[k] = data_out[k].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    data_out.to_sql('PROCEDURES', conn, if_exists='replace', index=False)\n",
    "\n",
    "if 'px' in db:\n",
    "    # PRESCRIPTIONS\n",
    "    print('Processing PRESCRIPTIONS')\n",
    "    data_input = pandas.read_csv(os.path.join(data_dir, \"PRESCRIPTIONS.csv\"))\n",
    "#     data_input = pandas.read_csv(os.path.join(out_dir, \"PRESCRIPTIONS.csv\"))\n",
    "    data_filter = []\n",
    "    cnt = 0\n",
    "    for itm in sampled_id:\n",
    "        msg = 'HADM_ID=='+str(itm)\n",
    "        data_filter.append(data_input.query(msg))\n",
    "        cnt += 1\n",
    "        show_progress(cnt, len(sampled_id))\n",
    "    data_out = pandas.concat(data_filter, ignore_index=True)\n",
    "    for k, v in data_out.dtypes.items():\n",
    "        data_out[k] = data_out[k].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    data_out.to_sql('PRESCRIPTIONS', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "if 'lab' in db:\n",
    "    # LAB\n",
    "    print('Processing LAB')\n",
    "    data_input = pandas.read_csv(os.path.join(data_dir, \"LAB.csv\"))\n",
    "    data_filter = []\n",
    "    cnt = 0\n",
    "    for itm in sampled_id:\n",
    "        msg = 'HADM_ID=='+str(itm)\n",
    "        data_filter.append(data_input.query(msg))\n",
    "        cnt += 1\n",
    "        show_progress(cnt, len(sampled_id))\n",
    "    data_out = pandas.concat(data_filter, ignore_index=True)\n",
    "    for k, v in data_out.dtypes.items():\n",
    "        data_out[k] = data_out[k].apply(lambda x: x.lower() if type(x) == str else x)\n",
    "    data_out.to_sql('LAB', conn, if_exists='replace', index=False)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caoyu/project/MultiModalMed/preprocessing/legacy/13-01-2021/utils/schema_mimic.py:88: UserWarning: Code: datetime is not defined in namespace XSD\n",
      "  'DISCHTIME':XSD.datetime  # hadm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PJT_ROOT_PATH:  result-dxprx\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 434178 entries, 0 to 434177\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   DIAGNOSES    434178 non-null  int64 \n",
      " 1   SUBJECT_ID   434178 non-null  int64 \n",
      " 2   HADM_ID      434178 non-null  int64 \n",
      " 3   ICD9_CODE    434178 non-null  object\n",
      " 4   SHORT_TITLE  434178 non-null  object\n",
      " 5   LONG_TITLE   434178 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 19.9+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 434178 entries, 0 to 434177\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   SUBJECT_ID  434178 non-null  int64 \n",
      " 1   DIAGNOSES   434178 non-null  int64 \n",
      " 2   HADM_ID     434178 non-null  int64 \n",
      " 3   ICD9_CODE   434178 non-null  object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 13.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6035 entries, 0 to 6034\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ICD9_CODE   6035 non-null   object\n",
      " 1   LONG_TITLE  6035 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 94.4+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147868 entries, 0 to 147867\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   PROCEDURES   147868 non-null  int64 \n",
      " 1   SUBJECT_ID   147868 non-null  int64 \n",
      " 2   HADM_ID      147868 non-null  int64 \n",
      " 3   ICD9_CODE    147868 non-null  int64 \n",
      " 4   SHORT_TITLE  147868 non-null  object\n",
      " 5   LONG_TITLE   147868 non-null  object\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 6.8+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147868 entries, 0 to 147867\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype\n",
      "---  ------      --------------   -----\n",
      " 0   SUBJECT_ID  147868 non-null  int64\n",
      " 1   PROCEDURES  147868 non-null  int64\n",
      " 2   HADM_ID     147868 non-null  int64\n",
      " 3   ICD9_CODE   147868 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 4.5 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1828 entries, 0 to 1827\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   ICD9_CODE   1828 non-null   int64 \n",
      " 1   LONG_TITLE  1828 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 28.7+ KB\n",
      "LOAD DB ...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "import os\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "from utils.schema_mimic import *\n",
    "from utils.evaluation.utils import query\n",
    "\n",
    "# PJT_ROOT_PATH = './'\n",
    "print('PJT_ROOT_PATH: ',PJT_ROOT_PATH)\n",
    "\n",
    "# db_conn = sqlite3.connect(os.path.join(PJT_ROOT_PATH, 'mimic_db/mimic.db'))\n",
    "db_conn = sqlite3.connect(os.path.join(out_dir, 'mimic.db'))\n",
    "conn = sqlite3.connect(os.path.join(out_dir , 'mimicsqlstar.db')) \n",
    "'''\n",
    "patient_cols = list(patient_demographic_dtype.keys())\n",
    "addmission_cols = list(hadm_demographic_dtype.keys())\n",
    "\n",
    "demographic = pd.read_sql_query(\"SELECT * FROM DEMOGRAPHIC\", db_conn)\n",
    "demographic.info()\n",
    "demographic.head()\n",
    "\n",
    "patients_df = demographic.loc[:,patient_cols]\n",
    "patients_df.info()\n",
    "\n",
    "addmissions_df = demographic.loc[:,addmission_cols] # primary key: HADM_ID\n",
    "addmissions_df.info()\n",
    "'''\n",
    "if 'dx' in db:\n",
    "    diagenoses = pd.read_sql_query(\"SELECT * FROM DIAGNOSES\", db_conn)\n",
    "    diagenoses = diagenoses.reset_index().rename({'index': 'DIAGNOSES'}, axis=1)\n",
    "    diagenoses.info()\n",
    "\n",
    "    diagnoses_cols = list(diagnoses_dtype.keys())\n",
    "    d_icd_dagnoses_cols = list(d_icd_diagnoses_dtype.keys())\n",
    "    diagnoses_cols = ['ICD9_CODE' if c == 'DIAGNOSES_ICD9_CODE' else c for c in diagnoses_cols]\n",
    "    d_icd_dagnoses_cols = ['ICD9_CODE' if c == 'DIAGNOSES_ICD9_CODE' else c for c in d_icd_dagnoses_cols]\n",
    "    d_icd_dagnoses_cols = ['LONG_TITLE' if c == 'DIAGNOSES_LONG_TITLE' else c for c in d_icd_dagnoses_cols]\n",
    "    #d_icd_dagnoses_cols = ['SHORT_TITLE' if c == 'DIAGNOSES_SHORT_TITLE' else c for c in d_icd_dagnoses_cols]\n",
    "\n",
    "    diagenoses_df = diagenoses.loc[:, diagnoses_cols]\n",
    "    diagenoses_df.info()\n",
    "\n",
    "    d_icd_diagenoses_df = diagenoses.loc[:, d_icd_dagnoses_cols]\n",
    "    d_icd_diagenoses_df.drop_duplicates(inplace=True)\n",
    "    d_icd_diagenoses_df.reset_index(inplace=True, drop=True)\n",
    "    d_icd_diagenoses_df.info()\n",
    "    \n",
    "    diagenoses_df.to_sql('DIAGNOSES', conn, if_exists='replace', index=False)\n",
    "    d_icd_diagenoses_df.to_sql('D_ICD_DIAGNOSES', conn, if_exists='replace', index=False)\n",
    "    \n",
    "if 'prx' in db:\n",
    "    procedures = pd.read_sql_query(\"SELECT * FROM PROCEDURES\", db_conn)\n",
    "    procedures = procedures.reset_index().rename({'index': 'PROCEDURES'}, axis=1)\n",
    "    procedures.info()\n",
    "\n",
    "    procedures_cols = list(procedures_dtype.keys())\n",
    "    d_icd_procedures_cols = list(d_icd_procedures_dtype.keys())\n",
    "    procedures_cols = ['ICD9_CODE' if c == 'PROCEDURES_ICD9_CODE' else c for c in procedures_cols]\n",
    "    d_icd_procedures_cols = ['ICD9_CODE' if c == 'PROCEDURES_ICD9_CODE' else c for c in d_icd_procedures_cols]\n",
    "    d_icd_procedures_cols = ['LONG_TITLE' if c == 'PROCEDURES_LONG_TITLE' else c for c in d_icd_procedures_cols]\n",
    "    #d_icd_procedures_cols = ['SHORT_TITLE' if c == 'PROCEDURES_SHORT_TITLE' else c for c in d_icd_procedures_cols]\n",
    "\n",
    "    procedures_df = procedures.loc[:, procedures_cols]\n",
    "    procedures_df.info()\n",
    "\n",
    "    d_icd_procedures_df = procedures.loc[:, d_icd_procedures_cols]\n",
    "    d_icd_procedures_df.drop_duplicates(inplace=True)\n",
    "    d_icd_procedures_df.reset_index(inplace=True, drop=True)\n",
    "    d_icd_procedures_df.info()\n",
    "    \n",
    "    procedures_df.to_sql('PROCEDURES', conn, if_exists='replace', index=False)\n",
    "    d_icd_procedures_df.to_sql('D_ICD_PROCEDURES', conn, if_exists='replace', index=False)\n",
    "    \n",
    "if 'lab' in db:\n",
    "    lab_cols = list(lab_dtype.keys())\n",
    "    d_labitem_cols = list(d_labitem_dtype.keys())\n",
    "\n",
    "    lab = pd.read_sql_query(\"SELECT * FROM LAB\", db_conn)\n",
    "    lab = lab.reset_index().rename({'index': 'LAB'}, axis=1)\n",
    "    lab.info()\n",
    "\n",
    "    lab_df = lab.loc[:, lab_cols]\n",
    "    lab_df.info()\n",
    "\n",
    "    d_labitem_df = lab.loc[:, d_labitem_dtype]\n",
    "    d_labitem_df.drop_duplicates(inplace=True)\n",
    "    d_labitem_df.reset_index(inplace=True, drop=True)\n",
    "    d_labitem_df.info()\n",
    "    \n",
    "    lab_df.to_sql('LAB', conn, if_exists='replace', index=False)\n",
    "    d_labitem_df.to_sql('D_LABITEM', conn, if_exists='replace', index=False)\n",
    "    \n",
    "if 'px' in db:\n",
    "    prescriptions_cols = list(prescriptions_dtype.keys())\n",
    "\n",
    "    prescriptions = pd.read_sql_query(\"SELECT * FROM PRESCRIPTIONS\", db_conn)\n",
    "    prescriptions = prescriptions.reset_index().rename({'index': 'PRESCRIPTIONS'}, axis=1)\n",
    "    prescriptions_df = prescriptions.loc[:, prescriptions_cols]\n",
    "    prescriptions_df.info()\n",
    "    \n",
    "    prescriptions_df.to_sql('PRESCRIPTIONS', conn, if_exists='replace', index=False)\n",
    "\n",
    "\n",
    "'''\n",
    "patients_df.to_sql('PATIENTS', conn, if_exists='replace', index=False)\n",
    "addmissions_df.to_sql('ADMISSIONS', conn, if_exists='replace', index=False)\n",
    "'''\n",
    "print(f'LOAD DB ...')\n",
    "\n",
    "db_file = os.path.join(out_dir,'mimicsqlstar.db')\n",
    "new_model = query(db_file)\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 434178 entries, 0 to 434177\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   DIAGNOSES    434178 non-null  int64 \n",
      " 1   SUBJECT_ID   434178 non-null  int64 \n",
      " 2   HADM_ID      434178 non-null  int64 \n",
      " 3   ICD9_CODE    434178 non-null  object\n",
      " 4   SHORT_TITLE  434178 non-null  object\n",
      " 5   LONG_TITLE   434178 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 19.9+ MB\n"
     ]
    }
   ],
   "source": [
    "diagenoses = pd.read_sql_query(\"SELECT * FROM DIAGNOSES\", db_conn)\n",
    "diagenoses = diagenoses.reset_index().rename({'index': 'DIAGNOSES'}, axis=1)\n",
    "diagenoses.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PJT_ROOT_PATH:  result-dxprx\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 434178 entries, 0 to 434177\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count   Dtype \n",
      "---  ------               --------------   ----- \n",
      " 0   SUBJECT_ID           434178 non-null  int64 \n",
      " 1   DIAGNOSES            434178 non-null  int64 \n",
      " 2   HADM_ID              434178 non-null  int64 \n",
      " 3   DIAGNOSES_ICD9_CODE  434178 non-null  object\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 13.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6035 entries, 0 to 6034\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   DIAGNOSES_ICD9_CODE   6035 non-null   object\n",
      " 1   DIAGNOSES_LONG_TITLE  6035 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 94.4+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:18<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total triples : 868356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 10.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total triples : 874391\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 147868 entries, 0 to 147867\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count   Dtype\n",
      "---  ------                --------------   -----\n",
      " 0   SUBJECT_ID            147868 non-null  int64\n",
      " 1   PROCEDURES            147868 non-null  int64\n",
      " 2   HADM_ID               147868 non-null  int64\n",
      " 3   PROCEDURES_ICD9_CODE  147868 non-null  int64\n",
      "dtypes: int64(4)\n",
      "memory usage: 4.5 MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1828 entries, 0 to 1827\n",
      "Data columns (total 2 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   PROCEDURES_ICD9_CODE   1828 non-null   int64 \n",
      " 1   PROCEDURES_LONG_TITLE  1828 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 28.7+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:05<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total triples : 1170127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 34.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total triples : 1171955\n",
      "SAVE KG ...\n",
      "SAVE DONE\n",
      "LOAD TEST ...\n",
      "1171955\n",
      "0 (rdflib.term.URIRef('/diagnoses/290158'), rdflib.term.URIRef('/diagnoses_icd9_code'), rdflib.term.URIRef('/diagnoses_icd9_code/69515'))\n",
      "1 (rdflib.term.URIRef('/diagnoses/285005'), rdflib.term.URIRef('/diagnoses_icd9_code'), rdflib.term.URIRef('/diagnoses_icd9_code/25080'))\n",
      "2 (rdflib.term.URIRef('/hadm_id/132029'), rdflib.term.URIRef('/diagnoses'), rdflib.term.URIRef('/diagnoses/340908'))\n",
      "3 (rdflib.term.URIRef('/hadm_id/111806'), rdflib.term.URIRef('/procedures'), rdflib.term.URIRef('/procedures/59693'))\n",
      "4 (rdflib.term.URIRef('/hadm_id/148295'), rdflib.term.URIRef('/diagnoses'), rdflib.term.URIRef('/diagnoses/91647'))\n",
      "5 (rdflib.term.URIRef('/hadm_id/136210'), rdflib.term.URIRef('/procedures'), rdflib.term.URIRef('/procedures/94544'))\n",
      "LOAD DONE\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import gc\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "from rdflib import Graph, URIRef\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from rdflib import Literal\n",
    "from tqdm import tqdm\n",
    "from utils.kg_complex_schema import addmissions_dtype, patients_dtype, procedures_dtype, prescriptions_dtype,\\\n",
    "    diagnoses_dtype, lab_dtype, d_icd_procedures_dtype, d_icd_diagnoses_dtype, d_labitem_dtype\n",
    "\n",
    "# PJT_ROOT_PATH = './'\n",
    "print('PJT_ROOT_PATH: ', PJT_ROOT_PATH)\n",
    "\n",
    "domain = ''\n",
    "\n",
    "def isNoneNan(val):\n",
    "    if val is None:\n",
    "        return True\n",
    "\n",
    "    if (type(val) == str) and (val.lower() in ['none', 'nan']):\n",
    "        return True\n",
    "\n",
    "    if val != val:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def clean_text(val):\n",
    "    if type(val) == str:\n",
    "        val = val.replace(\"\\\\\", ' ')\n",
    "    return val\n",
    "\n",
    "\n",
    "def wrap2uri(obj, literal_type):\n",
    "    obj = obj.lower()\n",
    "    if literal_type == 'entity':\n",
    "        return URIRef(obj)\n",
    "\n",
    "    elif literal_type == 'relation':\n",
    "        return URIRef(obj)\n",
    "\n",
    "    else:\n",
    "        return Literal(clean_text(obj), datatype=literal_type)\n",
    "\n",
    "\n",
    "def table2triples(knowgraph,df, parent_col, subject_col, col_types):\n",
    "    #triples = []\n",
    "    for col_name, _ in tqdm(col_types.items()):\n",
    "\n",
    "        if col_name == parent_col:\n",
    "            # triples += [(wrap2uri(f'{domain}/{col_name}/{sub}', col_types[parent_col]),\n",
    "            #              wrap2uri(f'{domain}/{subject_col}', 'relation'),\n",
    "            #              wrap2uri(f'{domain}/{subject_col}/{obj}', col_types[subject_col]))\n",
    "            #             for (sub, obj) in zip(df[col_name], df[subject_col])]\n",
    "            for (sub, obj) in zip(df[col_name], df[subject_col]):\n",
    "                knowgraph.add((wrap2uri(f'{domain}/{col_name}/{sub}', col_types[parent_col]),\n",
    "                             wrap2uri(f'{domain}/{subject_col}', 'relation'),\n",
    "                             wrap2uri(f'{domain}/{subject_col}/{obj}', col_types[subject_col]))\n",
    "                            )\n",
    "            continue\n",
    "\n",
    "        if col_name == subject_col:\n",
    "            continue\n",
    "        for (sub, obj) in zip(df[subject_col], df[col_name]):\n",
    "            if not isNoneNan(obj):\n",
    "                knowgraph.add(\n",
    "                            (wrap2uri(f'{domain}/{subject_col}/{sub}', col_types[subject_col]),\n",
    "                             wrap2uri(f'{domain}/{col_name}', 'relation'),\n",
    "                             wrap2uri(f'{domain}/{col_name}/{obj}' if col_types[col_name] == 'entity' else f'{obj}',\n",
    "                                      col_types[col_name]))\n",
    "                             )\n",
    "\n",
    "    return knowgraph\n",
    "\n",
    "db_conn = sqlite3.connect(os.path.join(out_dir, 'mimicsqlstar.db'))\n",
    "kg = Graph()\n",
    "'''\n",
    "patients = pd.read_sql_query(\"SELECT * FROM PATIENTS\", db_conn)\n",
    "patients.info()\n",
    "\n",
    "admissions = pd.read_sql_query(\"SELECT * FROM ADMISSIONS\", db_conn)\n",
    "admissions.info()\n",
    "'''\n",
    "if 'dx' in db:\n",
    "    diagnoses = pd.read_sql_query(\"SELECT * FROM DIAGNOSES\", db_conn)\n",
    "    diagnoses = diagnoses.rename({'ICD9_CODE': 'DIAGNOSES_ICD9_CODE'}, axis=1)\n",
    "    diagnoses.info()\n",
    "\n",
    "    d_icd_diagnoses = pd.read_sql_query(\"SELECT * FROM D_ICD_DIAGNOSES\", db_conn)\n",
    "    d_icd_diagnoses = d_icd_diagnoses.rename({'ICD9_CODE': 'DIAGNOSES_ICD9_CODE',\n",
    "                                              #'SHORT_TITLE': 'DIAGNOSES_SHORT_TITLE',\n",
    "                                              'LONG_TITLE': 'DIAGNOSES_LONG_TITLE'}, axis=1)\n",
    "    d_icd_diagnoses.info()\n",
    "    kg = table2triples(kg,diagnoses, parent_col='HADM_ID', subject_col='DIAGNOSES', col_types=diagnoses_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "    kg = table2triples(kg,d_icd_diagnoses, parent_col='', subject_col='DIAGNOSES_ICD9_CODE',\n",
    "                         col_types=d_icd_diagnoses_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "    \n",
    "if 'prx' in db:\n",
    "    procedures = pd.read_sql_query(\"SELECT * FROM PROCEDURES\", db_conn)\n",
    "    procedures = procedures.rename({'ICD9_CODE': 'PROCEDURES_ICD9_CODE'}, axis=1)\n",
    "    procedures.info()\n",
    "\n",
    "    d_icd_procedures = pd.read_sql_query(\"SELECT * FROM D_ICD_PROCEDURES\", db_conn)\n",
    "    d_icd_procedures = d_icd_procedures.rename({'ICD9_CODE': 'PROCEDURES_ICD9_CODE',\n",
    "                                                #'SHORT_TITLE': 'PROCEDURES_SHORT_TITLE',\n",
    "                                                'LONG_TITLE': 'PROCEDURES_LONG_TITLE'}, axis=1)\n",
    "    d_icd_procedures.info()\n",
    "    kg = table2triples(kg,procedures, parent_col='HADM_ID', subject_col='PROCEDURES', col_types=procedures_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "\n",
    "    kg = table2triples(kg,d_icd_procedures, parent_col='', subject_col='PROCEDURES_ICD9_CODE',\n",
    "                             col_types=d_icd_procedures_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "    \n",
    "if 'px' in db:\n",
    "    prescriptions = pd.read_sql_query(\"SELECT * FROM PRESCRIPTIONS\", db_conn)\n",
    "    prescriptions['ICUSTAY_ID'] = prescriptions['ICUSTAY_ID'].apply(lambda x: str(x) if x == x else None)\n",
    "    prescriptions.info()\n",
    "    \n",
    "    kg = table2triples(kg,prescriptions, parent_col='HADM_ID', subject_col='PRESCRIPTIONS',\n",
    "                         col_types=prescriptions_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "\n",
    "\n",
    "if 'lab' in db:\n",
    "    lab = pd.read_sql_query(\"SELECT * FROM LAB\", db_conn)\n",
    "    lab.info()\n",
    "\n",
    "    d_labitem = pd.read_sql_query(\"SELECT * FROM D_LABITEM\", db_conn)\n",
    "    d_labitem.info()\n",
    "    \n",
    "    kg = table2triples(kg,lab, parent_col='HADM_ID', subject_col='LAB', col_types=lab_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "    kg = table2triples(kg,d_labitem, parent_col='', subject_col='ITEMID', col_types=d_labitem_dtype)\n",
    "    print('# total triples : {}'.format(len(kg)))\n",
    "    \n",
    "\n",
    "'''\n",
    "kg = table2triples(kg,patients, parent_col='', subject_col='SUBJECT_ID', col_types=patients_dtype)\n",
    "print('# total triples : {}'.format(len(kg)))\n",
    "# print(triples[:5])\n",
    "#print(triples[-5:])\n",
    "#print(len(triples))\n",
    "\n",
    "kg = table2triples(kg,admissions, parent_col='SUBJECT_ID', subject_col='HADM_ID',\n",
    "                         col_types=addmissions_dtype)\n",
    "print('# total triples : {}'.format(len(kg)))\n",
    "# print(triples[-5:])\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "q = \"\"\"select * where { ?subject_id </gender> \"f\"^^<http://www.w3.org/2001/XMLSchema#string> }\"\"\"\n",
    "print(f\"TEST QEURY... {q})\")\n",
    "qres = kg.query(q)\n",
    "print(\"-\" * 50)\n",
    "for res in qres:\n",
    "    val = '|'\n",
    "    for t in res:\n",
    "        val += str(t.toPython()) + '|\\t\\t|'\n",
    "    print(val[:-1])\n",
    "print()\n",
    "'''\n",
    "\n",
    "\n",
    "print('SAVE KG ...')\n",
    "kg.serialize('{}/mimic_sparqlstar_kg.xml'.format(out_dir), format='xml')\n",
    "print('SAVE DONE')\n",
    "\n",
    "print('LOAD TEST ...')\n",
    "kg = Graph()\n",
    "kg.parse('{}/mimic_sparqlstar_kg.xml'.format(out_dir), format='xml', publicID='/')\n",
    "\n",
    "print(len(kg))\n",
    "for i, t in enumerate(kg):\n",
    "    print(i, t)\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "print('LOAD DONE')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171955/1171955 [00:07<00:00, 155501.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630450\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, URIRef\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "def build_dict(triples, nodes, edges):\n",
    "    h, r, t = triples\n",
    "    #for (h,r,t) in triples:\n",
    "    if h not in nodes:\n",
    "        nodes[h]=1\n",
    "    else:\n",
    "        nodes[h]+=1\n",
    "    if t not in nodes:\n",
    "        nodes[t]=1\n",
    "    else:\n",
    "        nodes[t]+=1\n",
    "    if r not in edges:\n",
    "        edges[r]=1\n",
    "    else:\n",
    "        edges[r]+=1\n",
    "    return nodes, edges\n",
    "\n",
    "# triple 확인\n",
    "nodes = dict()\n",
    "edges = dict()\n",
    "for triple in tqdm(kg):\n",
    "    triples = [x.n3() for x in triple]\n",
    "    #print(triples)\n",
    "    nodes, edges = build_dict(triples, nodes, edges)\n",
    "#matching = [s for s in tqdm(list(nodes.keys())) if \"hadm_id\" in list(nodes.keys())]\n",
    "print(len(nodes))\n",
    "print(len(edges))\n",
    "\n",
    "# if os.path.exists(os.path.join(db)):\n",
    "#     shutil.rmtree(os.path.join(db))\n",
    "# os.mkdir(os.path.join(db))\n",
    "\n",
    "f = open(os.path.join(db,'node_dict'),'w')\n",
    "for node in list(nodes.keys()):\n",
    "    f.write('{}\\n'.format(node))\n",
    "f.close()\n",
    "\n",
    "g = open(os.path.join(db,'edge_dict'),'w')\n",
    "for edge in list(edges.keys()):\n",
    "    g.write('{}\\n'.format(edge))\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171955/1171955 [00:08<00:00, 142327.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make file & Build look-up table\n",
    "train2id = open(os.path.join(db,'train2id.txt'),'w')\n",
    "train2id.write(str(len(kg))+'\\n')\n",
    "node2id = open(os.path.join(db,'entity2id.txt'),'w')\n",
    "node_lookup = {k:v for (v,k) in enumerate(nodes)}\n",
    "node2id.write(str(len(nodes))+'\\n')\n",
    "edge2id = open(os.path.join(db,'relation2id.txt'),'w')\n",
    "edge_lookup = {k:v for (v,k) in enumerate(edges)}\n",
    "edge2id.write(str(len(edges))+'\\n')\n",
    "\n",
    "# Build Node lookup\n",
    "for (node, idx) in list(node_lookup.items()):\n",
    "    node2id.write('{}\\t{}\\n'.format(node, idx))\n",
    "node2id.close()\n",
    "\n",
    "# Build Edge lookup\n",
    "for (edge, idx) in list(edge_lookup.items()):\n",
    "    edge2id.write('{}\\t{}\\n'.format(edge, idx))\n",
    "edge2id.close()\n",
    "\n",
    "# Actual triple to id triple\n",
    "for triple in tqdm(kg):\n",
    "    triples = [x.n3() for x in triple]\n",
    "    train2id.write('{}\\t{}\\t{}\\n'.format(node_lookup[triples[0]],node_lookup[triples[2]],edge_lookup[triples[1]]))\n",
    "train2id.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kg_txt",
   "language": "python",
   "name": "torch_kg_txt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
