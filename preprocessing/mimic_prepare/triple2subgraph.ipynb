{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCzMtobduJtf"
   },
   "source": [
    "# **1. Run Depth First Search on KG** \n",
    "\n",
    "Root is an admission node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ROOT_DIR = 'dxprx'\n",
    "NUM_SPECIAL_TOKENS = 3\n",
    "eval_size = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Not unified abstract embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119438,
     "status": "ok",
     "timestamp": 1604240640911,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "M6P8d-FytHkS",
    "outputId": "535c80ac-9331-4297-9e8c-49ce8dbb5bf1",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caoyu/anaconda3/envs/torch_kg_txt/lib/python3.7/site-packages/spacy/util.py:865: UserWarning: [W095] Model 'en_core_sci_sm' (0.5.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.4.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing\n",
      "level:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1171955/1171955 [00:00<00:00, 1176789.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589909/1171955\n",
      "level:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 589909/589909 [00:00<00:00, 774983.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7863/589909\n",
      "level:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7863/7863 [00:00<00:00, 779904.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/7863\n",
      "# Depth First Search Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import spacy, scispacy\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def get_childs(subgraph, depth, heads):\n",
    "    temp_seq = list()\n",
    "    for head in heads:\n",
    "        temp_seq += subgraph[depth][head]\n",
    "    return temp_seq\n",
    "\n",
    "# Build dictionaries from file\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(h,t):r for h,t,r in triples}\n",
    "nodes = {x.split('\\t')[0]:x.split('\\t')[-1] for x in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in k}\n",
    "edges = {x.split()[0]:x.split()[1] for x in open(os.path.join(ROOT_DIR,'relation2id.txt')).read().splitlines()[1:]}\n",
    "\n",
    "# Extract Admission Nodes & Literals\n",
    "adm_node = list()\n",
    "for node in list(nodes.items()):\n",
    "    if 'hadm' in node[0]:\n",
    "        adm_node.append(node[1])\n",
    "        \n",
    "# Initialize subgraph\n",
    "subgraph_norel = [{node:list() for node in adm_node}]\n",
    "\n",
    "# Depth First Search\n",
    "print('start preprocessing')\n",
    "level = 0\n",
    "while len(triples)>0:\n",
    "    queue = list()\n",
    "    print('level:{}'.format(level))\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[0] in subgraph_norel[level]:\n",
    "            subgraph_norel[level][triple[0]].append(triple[1])\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            queue.append(triple)\n",
    "    print('{}/{}'.format(len(queue),len(triples)))\n",
    "    new_head = list()\n",
    "    for heads in list(subgraph_norel[level].values()):\n",
    "        new_head+=heads\n",
    "    subgraph_norel.append({k:list() for k in new_head})\n",
    "    triples = queue\n",
    "    level += 1\n",
    "    if level > 30:\n",
    "        break\n",
    "\n",
    "print('# Depth First Search Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build subgraph >>>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32696/32696 [00:01<00:00, 26231.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32919\n",
      "('167118', {'dx': 'COPD, Coronary Artery Disease/atypical angina (LAD 30%, RCA 30%, EF 63%), hypercholesterolemia, hypothyroidism, Hypertension, hiatal hernia, Cerebral Vascular Accident,s/p Motor Vehicle Colision-> head injury & rib fracture. TBM- s/p tracheoplasty.', 'prx': 'bronchoscopy 3/31,4/2,3, , , s/p trachealplasty percutaneous tracheostomy after failed extubation down size trach on to size 6 cuffless'})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32919/32919 [07:28<00:00, 73.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32915\n",
      "239\n",
      "num_literals : 7863\n"
     ]
    }
   ],
   "source": [
    "# Build subgraph\n",
    "print('Build subgraph >>>')\n",
    "subgraphs = dict()\n",
    "max_len = 238\n",
    "cnt = 0\n",
    "for head in tqdm(list(subgraph_norel[0].keys())):\n",
    "    depth=0\n",
    "    seq = [head]\n",
    "    heads = [head]\n",
    "    while depth<level:\n",
    "        heads = get_childs(subgraph_norel,depth,heads)\n",
    "        seq += heads\n",
    "        depth+=1\n",
    "    if len(seq)>max_len:\n",
    "        continue\n",
    "    else:\n",
    "        subgraphs[head]=[2]+[int(x)+NUM_SPECIAL_TOKENS for x in seq]+[0]*(max_len-len(seq))\n",
    "\n",
    "\n",
    "      \n",
    "        \n",
    "# Align subgraph and note, remove unmathced samples\n",
    "#aid = [nodes['</hadm_id/{}>'.format(x)] for x in open(os.path.join(ROOT_DIR,'p_hadm_ids.txt')).read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
    "#note = [x for x in torch.load(os.path.join(ROOT_DIR,'p_sections.txt'))if (len(x)>0)]\n",
    "# Load and preprocess note\n",
    "# ROOT_DIR = 'result-px'\n",
    "note_aid_pair = list()\n",
    "f = torch.load(os.path.join('result-{}'.format(ROOT_DIR),'p_sections'))\n",
    "print(len(f))\n",
    "print(f[0])\n",
    "for aid, note in tqdm(torch.load(os.path.join('result-{}'.format(ROOT_DIR),'p_sections'))):\n",
    "    try:\n",
    "        if nodes[f'</hadm_id/{aid}>'] in subgraphs:\n",
    "            note_refined = {header.replace('\"',''):' '.join([token.text for token in nlp(text.replace('\"',''))]) for header, text in note.items()}\n",
    "            note_aid_pair.append((nodes[f'</hadm_id/{aid}>'],note_refined))\n",
    "    except:\n",
    "        continue\n",
    "#print('{}/{}'.format(len(aid),len(adm_node)))\n",
    "#print(len(note))\n",
    "print(len(note_aid_pair))\n",
    "print(max(list(map(lambda x: len(x),list(subgraphs.values())))))\n",
    "print('num_literals : {}'.format(len(literals.items())))\n",
    "\n",
    "# Re-indexing nodes in current subgraph after filtering\n",
    "new_nodes = list()\n",
    "for head, note in note_aid_pair:\n",
    "    new_nodes += subgraphs[head]\n",
    "new_nodes = set(new_nodes)\n",
    "old2new = dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Build DB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-0. TVT split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32915"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(note_aid_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32915/32915 [00:00<00:00, 39417.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# Build Input\n",
    "if not os.path.exists('{}.db'.format(ROOT_DIR)):\n",
    "    DB = {'train':[],'valid':[],'test':[]}\n",
    "    for sample in tqdm(note_aid_pair):\n",
    "        split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "        if (len(DB[split])>=eval_size) and (split in ['valid', 'test']):\n",
    "            split = 'train'\n",
    "        DB[split].append(sample)\n",
    "    torch.save(DB,'{}.db'.format(ROOT_DIR))\n",
    "else:\n",
    "    DB = torch.load('{}.db'.format(ROOT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] set size : 28915\n",
      "[valid] set size : 2000\n",
      "[test] set size : 2000\n"
     ]
    }
   ],
   "source": [
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIoqHq_nueia"
   },
   "source": [
    "## 2-1-(1). Masked Literal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 280336,
     "status": "ok",
     "timestamp": 1604241423375,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "T8yG81c8tHkW",
    "outputId": "2b36e7fd-a267-4346-d04c-fb5d0fe2b7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] set size : 28915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28915/28915 [31:30<00:00, 15.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:08<00:00, 15.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:06<00:00, 15.76it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_NoKGenc'.format(ROOT_DIR)\n",
    "# triples = [x.split() for x in open(os.path.join('train2id.txt')).read().splitlines()[1:]]\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    for head, note in tqdm(DB[split]):\n",
    "        subgraph = subgraphs[head]\n",
    "        inputs.append(subgraph)\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['</hadm_id/128319>', '</diagnoses/357668>', '</diagnoses/357659>', '</diagnoses/357665>', '</diagnoses/357669>', '</diagnoses/357657>', '</diagnoses/357662>', '</diagnoses/357658>', '</diagnoses/357664>', '</diagnoses/357661>', '</diagnoses/357675>', '</diagnoses/357672>', '</diagnoses/357663>', '</diagnoses/357655>', '</diagnoses/357660>', '</diagnoses/357667>', '</diagnoses/357656>', '</diagnoses/357673>', '</diagnoses/357670>', '</diagnoses/357666>', '</diagnoses/357671>', '</procedures/125835>', '</diagnoses/357674>', '</diagnoses_icd9_code/45981>', '</diagnoses_icd9_code/3669>', '</diagnoses_icd9_code/2875>', '</diagnoses_icd9_code/40391>', '</diagnoses_icd9_code/41401>', '</diagnoses_icd9_code/27800>', '</diagnoses_icd9_code/2859>', '</diagnoses_icd9_code/42822>', '</diagnoses_icd9_code/70719>', '</diagnoses_icd9_code/56210>', '</diagnoses_icd9_code/v4502>', '</diagnoses_icd9_code/v4986>', '</diagnoses_icd9_code/2724>', '</diagnoses_icd9_code/496>', '</diagnoses_icd9_code/4280>', '</diagnoses_icd9_code/v5861>', '</diagnoses_icd9_code/5856>', '</diagnoses_icd9_code/42731>', '</diagnoses_icd9_code/v1582>', '</diagnoses_icd9_code/v4511>', '</procedures_icd9_code/3995>', '</diagnoses_icd9_code/6826>', '\"venous (peripheral) insufficiency, unspecified\"', '\"unspecified cataract\"', '\"thrombocytopenia, unspecified\"', '\"hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage v or end stage renal disease\"', '\"coronary atherosclerosis of native coronary artery\"', '\"obesity, unspecified\"', '\"anemia, unspecified\"', '\"chronic systolic heart failure\"', '\"ulcer of other part of lower limb\"', '\"diverticulosis of colon (without mention of hemorrhage)\"', '\"automatic implantable cardiac defibrillator in situ\"', '\"do not resuscitate status\"', '\"other and unspecified hyperlipidemia\"', '\"chronic airway obstruction, not elsewhere classified\"', '\"congestive heart failure, unspecified\"', '\"long-term (current) use of anticoagulants\"', '\"end stage renal disease\"', '\"atrial fibrillation\"', '\"personal history of tobacco use\"', '\"renal dialysis status\"', '\"hemodialysis\"', '\"cellulitis and abscess of leg, except foot\"']\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4465, 2706, 3358, 4616, 1514, 5129, 7593, 6816, 6216, 1340, 4563, 5312, 6270, 1611, 4598, 6001, 3349, 540, 7541, 7300, 1645, 3532, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('\"hemodialysis\"', '\"anemia, unspecified\"', 6), ('</procedures_icd9_code/3995>', '</diagnoses_icd9_code/56210>', 6), ('</diagnoses_icd9_code/4280>', '\"end stage renal disease\"', 6), ('</diagnoses/357664>', '</diagnoses_icd9_code/40391>', 6), ('</diagnoses_icd9_code/2875>', '</diagnoses/357665>', 0), ('</diagnoses_icd9_code/6826>', '</diagnoses/357674>', 0), ('</diagnoses/357666>', '</diagnoses_icd9_code/v1582>', 0)]\n",
      "text:\n",
      "{'dx': '  Cellulitis Atrial fibrillation , Chronic systolic heart failure , End stage renal disease on', 'prx': 'None'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "# id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join('entity2id.txt')).read().splitlines()[1:]}\n",
    "\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([id2entity[x] for x in v[IDX][1:] if x!=0])\n",
    "    elif k=='rc_index':\n",
    "        print([(id2entity[db['input'][IDX][h]],id2entity[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1-(3). Masked Literal Prediction, Graph Enc, UniKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] set size : 28915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28915/28915 [35:18<00:00, 13.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:24<00:00, 13.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:24<00:00, 13.87it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UniKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(subgraph)\n",
    "        # Append label\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder, 相当于GCN中的关联矩阵，描述图节点之间的关联关系\n",
    "        mask =  torch.eye(len(subgraph))\n",
    "        for head_idx, head in enumerate(subgraph):\n",
    "            for tail_idx, tail in enumerate(subgraph):\n",
    "                if head_idx>tail_idx:\n",
    "                    continue\n",
    "                elif (head==0) or (tail==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    if (head,tail) in node2edge:\n",
    "                        mask[(head_idx, tail_idx)]=1.0\n",
    "                        mask[(tail_idx, head_idx)]=1.0\n",
    "        masks.append(mask)\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        # Append note for text encoder\n",
    "        notes.append(note)\n",
    "            \n",
    "    db = {'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'text':notes,\n",
    "                'rc_index':rc_indeces}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['</hadm_id/128319>', '</diagnoses/357668>', '</diagnoses/357659>', '</diagnoses/357665>', '</diagnoses/357669>', '</diagnoses/357657>', '</diagnoses/357662>', '</diagnoses/357658>', '</diagnoses/357664>', '</diagnoses/357661>', '</diagnoses/357675>', '</diagnoses/357672>', '</diagnoses/357663>', '</diagnoses/357655>', '</diagnoses/357660>', '</diagnoses/357667>', '</diagnoses/357656>', '</diagnoses/357673>', '</diagnoses/357670>', '</diagnoses/357666>', '</diagnoses/357671>', '</procedures/125835>', '</diagnoses/357674>', '</diagnoses_icd9_code/45981>', '</diagnoses_icd9_code/3669>', '</diagnoses_icd9_code/2875>', '</diagnoses_icd9_code/40391>', '</diagnoses_icd9_code/41401>', '</diagnoses_icd9_code/27800>', '</diagnoses_icd9_code/2859>', '</diagnoses_icd9_code/42822>', '</diagnoses_icd9_code/70719>', '</diagnoses_icd9_code/56210>', '</diagnoses_icd9_code/v4502>', '</diagnoses_icd9_code/v4986>', '</diagnoses_icd9_code/2724>', '</diagnoses_icd9_code/496>', '</diagnoses_icd9_code/4280>', '</diagnoses_icd9_code/v5861>', '</diagnoses_icd9_code/5856>', '</diagnoses_icd9_code/42731>', '</diagnoses_icd9_code/v1582>', '</diagnoses_icd9_code/v4511>', '</procedures_icd9_code/3995>', '</diagnoses_icd9_code/6826>', '\"venous (peripheral) insufficiency, unspecified\"', '\"unspecified cataract\"', '\"thrombocytopenia, unspecified\"', '\"hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage v or end stage renal disease\"', '\"coronary atherosclerosis of native coronary artery\"', '\"obesity, unspecified\"', '\"anemia, unspecified\"', '\"chronic systolic heart failure\"', '\"ulcer of other part of lower limb\"', '\"diverticulosis of colon (without mention of hemorrhage)\"', '\"automatic implantable cardiac defibrillator in situ\"', '\"do not resuscitate status\"', '\"other and unspecified hyperlipidemia\"', '\"chronic airway obstruction, not elsewhere classified\"', '\"congestive heart failure, unspecified\"', '\"long-term (current) use of anticoagulants\"', '\"end stage renal disease\"', '\"atrial fibrillation\"', '\"personal history of tobacco use\"', '\"renal dialysis status\"', '\"hemodialysis\"', '\"cellulitis and abscess of leg, except foot\"']\n",
      "mask:\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.])\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4465, 2706, 3358, 4616, 1514, 5129, 7593, 6816, 6216, 1340, 4563, 5312, 6270, 1611, 4598, 6001, 3349, 540, 7541, 7300, 1645, 3532, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "text:\n",
      "{'dx': '  Cellulitis Atrial fibrillation , Chronic systolic heart failure , End stage renal disease on', 'prx': 'None'}\n",
      "rc_index:\n",
      "[('\"anemia, unspecified\"', '</diagnoses/357674>', 6), ('</diagnoses/357668>', '\"hemodialysis\"', 6), ('</diagnoses/357659>', '\"chronic airway obstruction, not elsewhere classified\"', 6), ('</diagnoses/357675>', '</diagnoses_icd9_code/v4511>', 6), ('</diagnoses_icd9_code/496>', '\"chronic airway obstruction, not elsewhere classified\"', 5), ('</diagnoses_icd9_code/2724>', '\"other and unspecified hyperlipidemia\"', 5), ('</hadm_id/128319>', '</diagnoses/357662>', 1)]\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "# id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join('entity2id.txt')).read().splitlines()[1:]}\n",
    "\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([id2entity[x] for x in v[IDX][1:] if x!=0])\n",
    "    elif k=='mask':\n",
    "        print(v[IDX][1])\n",
    "    elif k=='rc_index':\n",
    "        print([(id2entity[db['input'][IDX][h]],id2entity[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2-(1). Unified Abstract Embedding, NoKGenc\n",
    "### 指的是graph和text的Abstract Embedding进行统一编码表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unified nodes : 7871\n",
      "[train] set size : 28915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28915/28915 [30:33<00:00, 15.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:04<00:00, 16.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:03<00:00, 16.23it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UnifiedNoKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "# triples = [x.split() for x in open(os.path.join('train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "#literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "#torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "# Abstract Node Unification\n",
    "node2uninode = {k:k for k in range(NUM_SPECIAL_TOKENS)}\n",
    "if ROOT_DIR == 'px':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'prescript':4,'icustay':5}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)\n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'prescript' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['prescript']\n",
    "        elif 'icustay' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['icustay']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "elif ROOT_DIR == 'dxprx':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'diagnoses_icd9_code':4,'diagnoses':5,'procedures_icd9_code':6,'procedures':7}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)     \n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'diagnoses_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses_icd9_code']\n",
    "        elif 'diagnoses' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses']\n",
    "        elif 'procedures_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures_icd9_code']\n",
    "        elif 'procedures' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures']\n",
    "        else:\n",
    "            print(key)\n",
    "            raise ValueError()\n",
    "\n",
    "torch.save(unified_node,'{}/unified_node'.format(task))\n",
    "torch.save(node2uninode,'{}/node2uninode'.format(task))\n",
    "print('# Unified nodes : {}'.format(len(unified_node)))\n",
    "            \n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    for head, note in tqdm(DB[split]):\n",
    "        subgraph = subgraphs[head]\n",
    "        inputs.append(list(map(lambda x: node2uninode[x],subgraph)))\n",
    "        labels.append(list(map(lambda x: node2uninode[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['CLS', 'hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures', 'diagnoses', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'procedures_icd9_code', 'diagnoses_icd9_code', '\"venous (peripheral) insufficiency, unspecified\"', '\"unspecified cataract\"', '\"thrombocytopenia, unspecified\"', '\"hypertensive chronic kidney disease, unspecified, with chronic kidney disease stage v or end stage renal disease\"', '\"coronary atherosclerosis of native coronary artery\"', '\"obesity, unspecified\"', '\"anemia, unspecified\"', '\"chronic systolic heart failure\"', '\"ulcer of other part of lower limb\"', '\"diverticulosis of colon (without mention of hemorrhage)\"', '\"automatic implantable cardiac defibrillator in situ\"', '\"do not resuscitate status\"', '\"other and unspecified hyperlipidemia\"', '\"chronic airway obstruction, not elsewhere classified\"', '\"congestive heart failure, unspecified\"', '\"long-term (current) use of anticoagulants\"', '\"end stage renal disease\"', '\"atrial fibrillation\"', '\"personal history of tobacco use\"', '\"renal dialysis status\"', '\"hemodialysis\"', '\"cellulitis and abscess of leg, except foot\"']\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4473, 2714, 3366, 4624, 1522, 5137, 7601, 6824, 6224, 1348, 4571, 5320, 6278, 1619, 4606, 6009, 3357, 548, 7549, 7308, 1653, 3540, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('diagnoses', 'diagnoses_icd9_code', 6), ('diagnoses', 'procedures', 6), ('diagnoses_icd9_code', 'diagnoses_icd9_code', 6), ('diagnoses', '\"coronary atherosclerosis of native coronary artery\"', 6), ('hadm', 'diagnoses', 1), ('hadm', 'diagnoses', 1), ('hadm', 'diagnoses', 1)]\n",
      "text:\n",
      "{'dx': '  Cellulitis Atrial fibrillation , Chronic systolic heart failure , End stage renal disease on', 'prx': 'None'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 1\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "uninode2name = {v:k.split('^^')[0] for k,v in unified_node.items()}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([uninode2name[x] for x in v[IDX] if x!=0])\n",
    "    elif k=='rc_index':\n",
    "        print([(uninode2name[db['input'][IDX][h]],uninode2name[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2-(2). Unified Abstract Embedding, UniKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Unified nodes : 7871\n",
      "[train] set size : 28915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28915/28915 [35:57<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[valid] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:26<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] set size : 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [02:24<00:00, 13.88it/s]\n"
     ]
    }
   ],
   "source": [
    "task = '{}_UnifiedUniKGenc'.format(ROOT_DIR)\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):int(r) for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "# Abstract Node Unification\n",
    "node2uninode = {k:k for k in range(NUM_SPECIAL_TOKENS)}\n",
    "if ROOT_DIR == 'px':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'prescript':4,'icustay':5}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)\n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'prescript' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['prescript']\n",
    "        elif 'icustay' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['icustay']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "\n",
    "elif ROOT_DIR == 'dxprx':\n",
    "    unified_node= {'PAD':0,'MASK':1,'CLS':2,'hadm':3,'diagnoses_icd9_code':4,'diagnoses':5,'procedures_icd9_code':6,'procedures':7}\n",
    "    for key in nodes:\n",
    "        if key in literals:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = len(unified_node)\n",
    "            unified_node[key]=len(unified_node)     \n",
    "        elif 'hadm' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['hadm']\n",
    "        elif 'diagnoses_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses_icd9_code']\n",
    "        elif 'diagnoses' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['diagnoses']\n",
    "        elif 'procedures_icd9_code' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures_icd9_code']\n",
    "        elif 'procedures' in key:\n",
    "            node2uninode[int(nodes[key])+NUM_SPECIAL_TOKENS] = unified_node['procedures']\n",
    "        else:\n",
    "            raise ValueError()\n",
    "            \n",
    "torch.save(unified_node,'{}/unified_node'.format(task))\n",
    "torch.save(node2uninode,'{}/node2uninode'.format(task))\n",
    "print('# Unified nodes : {}'.format(len(unified_node)))\n",
    "\n",
    "for split in DB:\n",
    "    ## Debugging Purpose\n",
    "    #if split == 'train':\n",
    "    #    continue\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    rc_indeces = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(list(map(lambda x: node2uninode[x],subgraph)))\n",
    "        # Append label\n",
    "        labels.append(list(map(lambda x: node2uninode[x] if x in literal_id2label else -100,subgraph)))\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder\n",
    "        mask =  torch.eye(len(subgraph))\n",
    "        for head_idx, head in enumerate(subgraph):\n",
    "            for tail_idx, tail in enumerate(subgraph):\n",
    "                if head_idx>tail_idx:\n",
    "                    continue\n",
    "                elif (head==0) or (tail==0):\n",
    "                    continue\n",
    "                else:\n",
    "                    if (head,tail) in node2edge:\n",
    "                        mask[(head_idx, tail_idx)]=1.0\n",
    "                        mask[(tail_idx, head_idx)]=1.0\n",
    "        # Add RC index for sample\n",
    "        num_nodes = sum([1 for x in subgraph if x!=0])\n",
    "        rc_index = list()\n",
    "        not_conn = 0\n",
    "        while len(rc_index) < 0.1*num_nodes:\n",
    "            idx_pair = (random.randint(0,num_nodes-1),random.randint(0,num_nodes-1))\n",
    "            node_pair = (subgraph[idx_pair[0]], subgraph[idx_pair[1]])\n",
    "            inv_node_pair = (subgraph[idx_pair[1]], subgraph[idx_pair[0]])\n",
    "            if (idx_pair[0] == idx_pair[1]):\n",
    "                continue\n",
    "            if node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[node_pair],))\n",
    "            elif inv_node_pair in node2edge:\n",
    "                rc_index.append(idx_pair+(node2edge[inv_node_pair],))\n",
    "            else:\n",
    "                if not_conn < 0.05*num_nodes:\n",
    "                    rc_index.append(idx_pair+(len(set(node2edge.values())),))\n",
    "                    not_conn +=1\n",
    "        rc_indeces.append(rc_index)\n",
    "        masks.append(mask)\n",
    "        notes.append(note)\n",
    "    db = {'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'rc_index':rc_indeces,\n",
    "                'text':notes}\n",
    "    torch.save(db,'{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7863"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(literal_id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:\n",
      "['CLS', 'hadm', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'procedures', 'procedures', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'procedures_icd9_code', 'procedures_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', 'diagnoses_icd9_code', '\"acute kidney failure, unspecified\"', '\"hyperpotassemia\"', '\"other and unspecified hyperlipidemia\"', '\"other postoperative infection\"', '\"surgical operation with anastomosis, bypass, or graft, with natural or artificial tissues used as implant causing abnormal patient reaction, or later complication, without mention of misadventure at time of operation\"', '\"cephalosporin group causing adverse effects in therapeutic use\"', '\"infection and inflammatory reaction due to cardiac device, implant, and graft\"', '\"anxiety state, unspecified\"', '\"other chronic pain\"', '\"hematuria, unspecified\"', '\"unspecified accident\"', '\"thoracentesis\"', '\"thoracentesis\"', '\"surgical operation with implant of artificial internal device causing abnormal patient reaction, or later complication,without mention of misadventure at time of operation\"', '\"acute on chronic systolic heart failure\"', '\"acute respiratory failure\"', '\"injury to bladder and urethra, without mention of open wound into cavity\"', '\"coronary atherosclerosis of unspecified type of vessel, native or graft\"', '\"congestive heart failure, unspecified\"', '\"unspecified essential hypertension\"', '\"cellulitis and abscess of trunk\"', '\"aortocoronary bypass status\"', '\"unspecified pleural effusion\"', '\"other specified hypersensitivity angiitis\"', '\"methicillin susceptible staphylococcus aureus in conditions classified elsewhere and of unspecified site\"', '\"acidosis\"']\n",
      "mask:\n",
      "tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0.])\n",
      "label:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 2596, 7146, 6278, 2398, 2443, 5131, 5946, 5759, 116, 5130, 1359, 4953, 4953, 3717, 5527, 4406, 4811, 3332, 4606, 1931, 7355, 7123, 6156, 2322, 3255, 7253, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "label_mask:\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "rc_index:\n",
      "[('CLS', '\"acute on chronic systolic heart failure\"', 6), ('\"cephalosporin group causing adverse effects in therapeutic use\"', 'diagnoses_icd9_code', 6), ('\"other and unspecified hyperlipidemia\"', 'diagnoses', 6), ('procedures_icd9_code', '\"thoracentesis\"', 4), ('diagnoses_icd9_code', 'diagnoses', 6), ('hadm', 'diagnoses', 1), ('procedures', 'procedures_icd9_code', 3), ('diagnoses_icd9_code', '\"other chronic pain\"', 5)]\n",
      "text:\n",
      "{'dx': 'Acute renal failure Vasculitis , likely cephalosporin-related Acute on chronic systolic congestive heart failure MSSA sternal wound infection', 'prx': 'Bilateral thoracenteses S/p removal of chest wall drain'}\n"
     ]
    }
   ],
   "source": [
    "IDX = 10\n",
    "id2entity = {int(line.split('\\t')[1])+NUM_SPECIAL_TOKENS:line.split('\\t')[0].split('^^')[0] for line in open(os.path.join(ROOT_DIR,'entity2id.txt')).read().splitlines()[1:]}\n",
    "uninode2name = {v:k.split('^^')[0] for k,v in unified_node.items()}\n",
    "for k, v in db.items():\n",
    "    print(f'{k}:')\n",
    "    if k=='input':\n",
    "        print([uninode2name[x] for x in v[IDX] if x!=0])\n",
    "    elif k=='mask':\n",
    "        print(v[IDX][1])\n",
    "    elif k=='rc_index':\n",
    "        print([(uninode2name[db['input'][IDX][h]],uninode2name[db['input'][IDX][t]],r) for h,t,r in v[IDX]])\n",
    "    else:\n",
    "        print(v[IDX])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xo4WacHCveOi"
   },
   "source": [
    "## 2-3. Literal Bucket Prediction _(ongoing..)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nUWX_Dxg1jCH"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'literalID2bucketID'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_51709/4271049560.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;31m# Load Bucket ID\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m \u001B[0mliteralID2bucketID\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'literalID2bucketID'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0msplit\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mDB\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch_kg_txt/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001B[0m\n\u001B[1;32m    769\u001B[0m         \u001B[0mpickle_load_args\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'encoding'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m'utf-8'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    770\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 771\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'rb'\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    772\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    773\u001B[0m             \u001B[0;31m# The zipfile reader is going to advance the current file position.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch_kg_txt/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    268\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    269\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 270\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    271\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    272\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;34m'w'\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/torch_kg_txt/lib/python3.7/site-packages/torch/serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[0;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'literalID2bucketID'"
     ]
    }
   ],
   "source": [
    "task = 'masked_literal_prediction'\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "# Build Input\n",
    "DB = {'train':[],'valid':[],'test':[]}\n",
    "for sample in note_aid_pair:\n",
    "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
    "        split = 'train'\n",
    "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
    "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
    "    DB[split].append(sample)\n",
    "\n",
    "# Load Bucket ID\n",
    "literalID2bucketID = torch.load('literalID2bucketID')\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    torch.save(literal_id2label,'{}/id2label'.format(os.path.join(task,split)))\n",
    "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(os.path.join(task,split)))\n",
    "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
    "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.values()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
    "                'label':[list(map(lambda x: literalID2bucketID[x] if x in literalID2bucketID else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
    "               '{}/{}/kg_norel'.format(task,split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cl5zew0jvW75"
   },
   "source": [
    "## 2-4. Contrastive Learning _(ongoing..)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNTHhbQZ1jaU"
   },
   "outputs": [],
   "source": [
    "def negative_sampling(input, mask):\n",
    "\n",
    "task = 'masked_literal_prediction'\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "# Build Input\n",
    "DB = {'train':[],'valid':[],'test':[]}\n",
    "for sample in note_aid_pair:\n",
    "    split = np.random.choice(list(DB.keys()),p=[0.8,0.1,0.1])\n",
    "    if (len(split)>0.1*len(note_aid_pair)) and (split in ['valid', 'test']):\n",
    "        split = 'train'\n",
    "    elif (len(split)>0.8*len(note_aid_pair)) and (split in ['train']):\n",
    "        split = np.random.choice(['valid','test'],p=[0.5,0.5])\n",
    "    DB[split].append(sample)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    torch.save(literal_id2label,'{}/id2label'.format(os.path.join(task,split)))\n",
    "    torch.save([note for (head,note) in DB[split]],'{}/note'.format(os.path.join(task,split)))\n",
    "    torch.save({'input':[subgraphs[head] for (head,note) in DB[split]],\n",
    "                'mask':[(~np.isin(np.array(subgraphs[head]),list(literals.values()))).astype(np.int64).tolist() for (head,note) in DB[split]],\n",
    "                'label':[list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[head])) for (head,note) in DB[split]]},\n",
    "               '{}/{}/kg_norel'.format(task,split))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3z4_vqY2rzc"
   },
   "source": [
    "**Supp 1. Save DB in torch.tensor format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwxPUhGr2ruw"
   },
   "outputs": [],
   "source": [
    "# Only for DB in tensor form\n",
    "# Get id sequence of notes\n",
    "\n",
    "print(subgraphs)\n",
    "tensorized_subgraphs = torch.LongTensor([x for x in subgraphs])\n",
    "print(max_len)\n",
    "print(len(subgraphs))\n",
    "print(tensorized_subgraphs[0,:20])\n",
    "print('Saving...')\n",
    "torch.save(tensorized_subgraphs,'subgraph_norel')\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supp 2. Check input for debugging purpose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 746,
     "status": "ok",
     "timestamp": 1604245224905,
     "user": {
      "displayName": "Park Sungjin",
      "photoUrl": "",
      "userId": "10892187777297360592"
     },
     "user_tz": -540
    },
    "id": "z6vOs_8HZCiy",
    "outputId": "3da879b2-d0e1-4b72-bfd1-8e6e3b63f6c6"
   },
   "outputs": [],
   "source": [
    "split = 'test'\n",
    "input = subgraphs[DB[split][0][0]]\n",
    "mask = (~np.isin(np.array(subgraphs[DB[split][0][0]]),list(literals.values()))).astype(np.int64).tolist() \n",
    "label = list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraphs[DB[split][0][0]])) \n",
    "print(input)\n",
    "print(mask)\n",
    "print(label)\n",
    "#print(literals)\n",
    "print(len(literals))\n",
    "print(list(literals.items())[-1])\n",
    "print(list(literal_id2label.items())[-1])\n",
    "#list(literals.values())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7awNjSEwl9E"
   },
   "source": [
    "**Supp 3. Run Depth First Search on KG (Node & Relation)**\n",
    "\n",
    "Root is an admission node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qME9ZZ1UwuSv"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "NUM_SPECIAL_TOKENS = 2\n",
    "\n",
    "def get_childs_withrel(subgraph, depth, heads,node2edge):\n",
    "    temp_seq = list()\n",
    "    temp_heads = list()\n",
    "    for head in heads:\n",
    "        node_set = [(head,tail) for tail in subgraph[depth][head]]\n",
    "        for node_pair in node_set:\n",
    "            temp_seq += ['r'+node2edge[node_pair],node_pair[1]]\n",
    "        temp_heads += subgraph[depth][head]\n",
    "    return temp_seq, temp_heads\n",
    "\n",
    "triples = [x.split() for x in open('train2id.txt').read().splitlines()[1:]]\n",
    "node2edge = {(h,t):r for h,t,r in triples}\n",
    "nodes = {' '.join(x.split()[:-1]):x.split()[-1] for x in open('entity2id.txt').read().splitlines()[1:]}\n",
    "literals = {k:int(v)+NUM_SPECIAL_TOKENS for (k,v) in list(nodes.items()) if '^^' in node[0]}\n",
    "edges = {x.split()[0]:x.split()[1] for x in open('relation2id.txt').read().splitlines()[1:]}\n",
    "\n",
    "# Extract Admission Nodes & Literals\n",
    "adm_node = list()\n",
    "for node in list(nodes.items()):\n",
    "    if 'hadm' in node[0]:\n",
    "        adm_node.append(node[1])   \n",
    "        \n",
    "# Initialize subgraph\n",
    "subgraph_norel = [{node:list() for node in adm_node}]\n",
    "\n",
    "#subgraph_rel = dict(adm_node)\n",
    "#node_dict = list(subgraph_norel.keys())\n",
    "\n",
    "# Depth First Search\n",
    "print('start preprocessing')\n",
    "level = 0\n",
    "while len(triples)>0:\n",
    "    queue = list()\n",
    "    print('level:{}'.format(level))\n",
    "    for triple in tqdm(triples):\n",
    "        if triple[0] in subgraph_norel[level]:\n",
    "            subgraph_norel[level][triple[0]].append(triple[1])\n",
    "            flag = False\n",
    "        else:\n",
    "            flag = True\n",
    "        if flag:\n",
    "            queue.append(triple)\n",
    "    print('{}/{}'.format(len(queue),len(triples)))\n",
    "    new_head = list()\n",
    "    for heads in list(subgraph_norel[level].values()):\n",
    "        new_head+=heads\n",
    "    subgraph_norel.append({k:list() for k in new_head})\n",
    "    triples = queue\n",
    "    level += 1\n",
    "\n",
    "# Build subgraph\n",
    "subgraphs = list()\n",
    "max_len = 0\n",
    "for head in tqdm(list(subgraph_norel[0].keys())):\n",
    "    depth=0\n",
    "    seq = [head]\n",
    "    heads = [head]\n",
    "    while depth<level:\n",
    "        seqs, heads = get_childs_withrel(subgraph_norel,depth,heads,node2edge)\n",
    "        seq += seqs\n",
    "        depth+=1\n",
    "    subgraphs.append(list(map(lambda x: int(x)+NUM_SPECIAL_TOKENS if 'r' not in x else -(int(x.split('r')[-1])+1),seq)))\n",
    "    if len(seq)>max_len:\n",
    "        max_len = len(seq)\n",
    "\n",
    "# Align subgraph and note\n",
    "aid = [nodes['</hadm_id/{}>'.format(x)] for x in open('p_hadm_ids.txt').read().splitlines() if (len(x)>0) and ('</hadm_id/{}>'.format(x) in nodes)]\n",
    "note = [x for x in open('p_sections.txt').read().splitlines() if (len(x)>0)]\n",
    "note_aid_pair = [(x,y) for (x,y) in zip(aid,note) if x in subgraphs]\n",
    "print('{}/{}'.format(len(aid),len(adm_node)))\n",
    "print(len(note))\n",
    "print(len(note_aid_pair))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1-(2). Masked Literal Prediction, Graph Enc, MultiKGenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = '{}_MultiKGenc'.format(ROOT_DIR)\n",
    "UniDirectional = False\n",
    "triples = [x.split() for x in open(os.path.join(ROOT_DIR,'train2id.txt')).read().splitlines()[1:]]\n",
    "node2edge = {(int(h)+NUM_SPECIAL_TOKENS,int(t)+NUM_SPECIAL_TOKENS):r for h,t,r in triples}\n",
    "\n",
    "if not os.path.isdir(task):\n",
    "    os.mkdir(task)\n",
    "\n",
    "# Re-index literals for labeling\n",
    "literal_id2label = {k:v for (v,k) in enumerate(list(literals.values()))}\n",
    "torch.save(literal_id2label,'{}/id2label'.format(task))\n",
    "\n",
    "for split in DB:\n",
    "    print('[{}] set size : {}'.format(split, len(DB[split])))\n",
    "    if not os.path.isdir(os.path.join(task,split)):\n",
    "        os.mkdir(os.path.join(task,split))\n",
    "    inputs = list()\n",
    "    masks = list()\n",
    "    labels = list()\n",
    "    label_masks = list()\n",
    "    notes = list()\n",
    "    \n",
    "    for head, note in tqdm(DB[split],total=len(DB[split])):\n",
    "        subgraph = subgraphs[head]\n",
    "        # Append input\n",
    "        inputs.append(subgraph)\n",
    "        # Append label\n",
    "        # --- literal_id2label 将graph中的id号，重新统一编码，从1开始到len(literal)\n",
    "        labels.append(list(map(lambda x: literal_id2label[x] if x in literal_id2label else -100,subgraph)))\n",
    "        # --- label_masks 如果在literal_id2label 中的位置为1，不在为0\n",
    "        label_masks.append(list(map(lambda x: 1 if x in literal_id2label else 0,subgraph)))\n",
    "        # Append attention mask for graph encoder\n",
    "        mask =  torch.stack([torch.eye(len(subgraph)) for _ in range(len(edges))],dim=2)\n",
    "        if UniDirectional:\n",
    "            for head_idx, head in enumerate(subgraph):\n",
    "                for tail_idx, tail in enumerate(subgraph):\n",
    "                    if head_idx>tail_idx:\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (head,tail) in node2edge:\n",
    "                            mask[(head_idx, tail_idx, node2edge[(head,tail)])]=1.0\n",
    "        else:\n",
    "            for head_idx, head in enumerate(subgraph):\n",
    "                for tail_idx, tail in enumerate(subgraph):\n",
    "                    if head_idx>tail_idx:\n",
    "                        continue\n",
    "                    elif (head==0) or (tail==0):\n",
    "                        continue\n",
    "                    else:\n",
    "                        if (head,tail) in node2edge:\n",
    "                            mask[(head_idx, tail_idx, int(node2edge[(head,tail)]))]=1.0\n",
    "                            mask[(tail_idx, head_idx, int(node2edge[(head,tail)]))]=1.0\n",
    "        masks.append(mask)\n",
    "        notes.append(note)\n",
    "            \n",
    "    torch.save({'input':inputs,\n",
    "                'mask':masks,\n",
    "                'label':labels,\n",
    "                'label_mask':label_masks,\n",
    "                'text':notes},\n",
    "                '{}/db'.format(os.path.join(task,split)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB['test'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for head, note in tqdm(DB['test'],total=len(DB['test'])):\n",
    "    print(head)\n",
    "    print(note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph = subgraphs['231386']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [torch.eye(len(subgraph)) for _ in range(len(edges))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tmp))\n",
    "print(len(tmp[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask =  torch.stack([torch.eye(len(subgraph)) for _ in range(len(edges))],dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if UniDirectional:\n",
    "    for head_idx, head in enumerate(subgraph):\n",
    "        for tail_idx, tail in enumerate(subgraph):\n",
    "            if head_idx>tail_idx:\n",
    "                continue\n",
    "            else:\n",
    "                if (head,tail) in node2edge:\n",
    "                    mask[(head_idx, tail_idx, node2edge[(head,tail)])]=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mask[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "triple2subgraph.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torch_kg_txt",
   "language": "python",
   "name": "torch_kg_txt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
